---
title: "P8105 Homework 5"
author: "Hyun Kim (hk3373)"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)

theme_set(theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5)))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
```

# Problem 1

## Function to randomly draw birthday, check duplicates and return true or false
```{r define_birthday_funtion}
sim_birthday = function(n) {

  birthdays = sample(1:365, size = n, replace = TRUE)
  duplicate = length(unique(birthdays)) < n
  
  return(duplicate)
  
}
```

## Run function 10000 times for each group size between 2 and 50
```{r birthday_simulation}
sim_birthday_results = 
  expand_grid(n = 2:50,
              iter = 1:10000) |> 
  mutate(birthday_result = map_lgl(n, sim_birthday)) |> 
  group_by(n) |> 
  summarize(probability = mean(birthday_result))
```

## Plot the probability as a function of group size
```{r birthday_vs_probability}
sim_birthday_results |> 
  ggplot(aes(x = n, y = probability)) + 
  geom_line()
```

Group size and the probability that at least two people in group will share a 
birthday shows a positive relationship. As the group size increases, the 
probability also increases, approaching the value of 1. 

# Problem 2

## Function to perform one sample t-test
```{r t_test_function}
sim_t_test = function(mu, n = 30, sigma = 5) {
  
  sim_data = rnorm(n, mean = mu, sd = sigma)
  
  t_test_result = 
    t.test(sim_data, 
           conf.levl = 1 - 0.05) |>
    broom::tidy() |>
    select(estimate, p.value)
  
  return(t_test_result)
}
```

## Generate 5000 datasets 
```{r simulate_datasets}
mu_zero_result =
  expand_grid(
    mu = 0,
    iter = 1:5000) |>
  mutate(test_result = map(mu, sim_t_test)) |>
  unnest(test_result)
```

## Repeat the above for true mean = {1, 2, 3, 4, 5, 6}
```{r t_test_simulations}
sim_test_results = 
  expand_grid(
    mu = 1:6,
    iter = 1:5000) |>
  mutate(test_result = map(mu, sim_t_test)) |>
  unnest(test_result) |>
  bind_rows(mu_zero_result)
```

## Plot proportion of times the null was rejected vs. true mean
```{r proportion_vs_mean}
sim_test_results |>
  group_by(mu) |>
  summarize(reject_prop = sum(p.value < 0.05) / n()) |>
  ggplot(aes(y = reject_prop, x = mu)) +
  geom_point() +
  geom_line() +
  labs(x = "True Mean",
       y = "Power of Test",
       title = "Power of Test vs. True Mean")
```

As the true mean increases, the power of the test increases, approaching the 
value of 1. Therefore, there is a positive association between true mean and 
power.

## Plot average estimate vs. true mean
```{r estimate_vs_mean}
sim_test_results |>
  group_by(mu) |>
  summarize(avg_estimate = mean(estimate)) |>
  ggplot(aes(y = avg_estimate, x = mu)) +
  geom_point() +
  geom_line() +
  labs(x = "True Mean",
       y = "Average Estimate",
       title = "Average Estimate vs. True Mean") 
```

## Plot average estimate (null rejected) vs. true mean
```{r rejected_estimate_vs_mean}
sim_test_results |>
  filter(p.value < 0.05) |>
  group_by(mu) |>
  summarize(avg_estimate = mean(estimate)) |>
  ggplot(aes(y = avg_estimate, x = mu)) +
  geom_point() +
  geom_line() +
  labs(x = "True Mean",
       y = "Average Estimate",
       title = "Average Estimate (Null Rejected) vs. True Mean")
```

The sample average of the estimate across tests for which the null is rejected 
is approximately equal to the true mean, since the data points and plot line 
reflect an estimated value that is close to the true mean.

# Problem 3

## Describe the raw data
```{r import_raw_data}
homicide_df = 
  read_csv("data/homicide-data.csv") |>
  janitor::clean_names()

str(homicide_df)
```

The dataset has `r nrow(homicide_df)` rows and `r ncol(homicide_df)` columns 
including uid, the victim's first name, last name, race, age, sex, city, state 
and disposition as character variables, and reported date, latitude and 
longitude as numeric variables. 

## Create city_state variable
```{r create_city_state}
homicide_df = 
  homicide_df |>
  mutate(city_state = str_c(city, state, sep = ", "))
```

## Summarize within cities to obtain total number of homicides and unsolved homicides
```{r summarize_num_homicides}
homicide_prop_df = 
  homicide_df |>
  group_by(city_state) |>
  summarize(
    total_homicides = n(),
    unsolved_homicides = 
      sum(disposition %in% c("Closed without arrest", "Open/No arrest"))) 
```

## Estimate the proportion of unsolved homicides for Baltimore, MD
```{r estimate_baltimore_proportion}
baltimore_homicide = 
  homicide_prop_df |>
  filter(city_state == "Baltimore, MD") 

baltimore_result_df =
  prop.test(x = pull(baltimore_homicide, unsolved_homicides),
            n = pull(baltimore_homicide, total_homicides)) |>
  broom::tidy()
```

## Pull the estimated proportion and confidence intervals of Baltimore
```{r pull_baltimore_results}
pull(baltimore_result_df, estimate)
pull(baltimore_result_df, conf.high)
pull(baltimore_result_df, conf.low)
```

## Define function and run prop.test for each city in the dataset
```{r prop_test_simulations}
homicide_prop_test = function(city) {
  
  homicide_city_df = 
      homicide_prop_df |>
      filter(city_state == city) 
  
  prop_test_result = 
    prop.test(x = pull(homicide_city_df, unsolved_homicides),
              n = pull(homicide_city_df, total_homicides)) |>
    broom::tidy() |>
    select(estimate, conf.low, conf.high)
  
  return(prop_test_result)
}

city_prop_results = 
  expand_grid(city = pull(homicide_prop_df, city_state)) |>
  mutate(prop_result = map(city, homicide_prop_test)) |>
  unnest(prop_result)

city_prop_results
```

## Plot estimates and CIs for each city 
```{r city_estimates_CIs, fig.width = 8, fig.height = 6}
city_prop_results |>
  ggplot(aes(y = estimate, x = reorder(city, estimate))) + 
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(y = "Estimate",
       x = "City", 
       title = "Estimate and Confidence Intervals of Unsolved Homicides by City")
```
